---
title: 'SONY Study Notes'
date: 2024-01-07
permalink: /posts/2024/01/blog-post-4/
tags:
  - Internship
  - knowledge
  - AI
---

SONY学习笔记
------

* * *
<details> <summary>

## Prompt Engineering

</summary>

code ...

</details>

## Prompt Engineering

Qwen + LLM Agent + ReAct Prompting
ReAct 框架允许 LLMs 与外部工具交互来获取额外信息，从而给出更可靠和实际的回应。

- Thought: 思考推理，其实就是做planning。
- Action：其实就是做行动，决定使用什么工具
- Action Input：工具输入
- Observation：工具输出

```python
messages = [
    {
        "role": Role.SYSTEM,
        "content": "You are an embodied AI assistant that controls a robot and interacts with the user.",
    },
    {"role": Role.USER, "content": "The robot are in an environment that "+circumstance+", Please reply 'Robot already knows the basic environment aorund robot' "},
]
response = Generation.call(
        # Generation.Models.qwen_max,
        model="qwen-max-1201",
        messages=messages,
        result_format="message",  # set the result to be "message" format.
        stop_tokens=stop,
        temperature=0.05,
    )
print(response.output.choices[0]['message']["content"])

messages.append({"role": Role.USER, "content": prompt_1})
```

```python
def tool_wrapper_for_qwen(tool):
    def tool_(query):
        query = json.loads(query)
        return tool(**query)

    return tool_
```

```python
TOOLS = [
    {
        "name_for_human": "search_object",
        "name_for_model": "search_object",
        "description_for_model": "Robot will get the location of queried object at the robot's current location. The location is described by x and y coordinates.",
        "parameters": [
            {
                "name": "object_name",
                "type": "string",
                "description": "the name of object to search",
                "required": True,
            }
        ],
        "tool_api": tool_wrapper_for_qwen(search_object),
    },
]
```

<https://zhuanlan.zhihu.com/p/631922240>
<https://www.promptingguide.ai/zh/introduction/tips>

- Zero-shot prompt: 零样本的prompt。此为最常见的使用形式。之所以叫zero-shot，是因为我们直接用大模型做任务而不给其参考示例。这也被视为评测大模型能力的重要场景之一。
- Few-shot prompt: 与zero-shot相对，在与大模型交互时，在prompt中给出少量示例。
  - 使用KNN等近邻算法去选择与test样本距离更近的few-shot example；
  - 随机使用few-shot example；
  - 使用强化学习或主动学习去进一步选择few-shot example。
- Role prompt: 与大模型玩“角色扮演”游戏。让大模想象自己是某方面专家、因而获得更好的任务效果。
- Instruction prompt: 指令形式的prompt。
- Chain-of-thought prompt: 常见于推理任务中，通过让大模型“Let's think step by step”来逐步解决较难的推理问题。
- Multimodal prompt: 多模态prompt。顾名思义，输入不再是单一模态的prompt，而是包含了众多模态的信息。如同时输入文本和图像与多模态大模型进行交互。

* * *

## Grounded-Segment-Anything

* * *

## Flask 封装 API

```python
@app.route("/<image_path>/<text>", methods=["GET"])
def locate_object(image_path, text):
    image_path = urllib.parse.unquote_plus(
        image_path
    )  # Decode the URL of the image path
    print("object: " + text)
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    annotated_image, centroid_x, centroid_y = grounded_sam(image, text)
    response_data = {"x": centroid_x, "y": centroid_y}
    return jsonify(response_data)
```

```python
resp = requests.get(LOCATE_OBJECT_API, timeout=15)
content = json.loads(resp.content)
```
Flask不允许返回None

https://www.cnblogs.com/cleven/p/10858016.html#d

https://zhuanlan.zhihu.com/p/137649301

### gradio (很好用)

```python
import gradio as gr
demo = gr.Interface(
    fn=grounded_sam,
    inputs=["image", "text"],
    outputs=["image", "text", "text"],
)
if __name__ == "__main__":
    demo.launch()
```

* * *

## computer vision 三维重建
